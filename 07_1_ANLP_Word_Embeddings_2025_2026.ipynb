{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PodevynLoris/Mobile_robot_simulator/blob/main/07_1_ANLP_Word_Embeddings_2025_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tNwXXJ_BBb"
      },
      "source": [
        "## Start by copying this into your Google Drive!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBbWrF5_DXC"
      },
      "source": [
        "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
        "\n",
        "# Advanced Natural Language Processing Course - Tutorial Word Embeddings\n",
        "Author: Jan Scholtes en Gijs Wijngaard\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Version 2025-2026.1\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "z1GuZIbMZGDK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xe-BeO_G34"
      },
      "source": [
        "Welcome to the tutorial about Word Embeddings, a key component of Deep-Learning NLP applications. In this notebook you will work with Word2Vec models, and we finish off with transformers and sentence transformers.\n",
        "\n",
        "We will also discuss several limitations of Word Embeddings. Like tokenization errors, wrong use of word embeddings will propagate through the entire NLP pipeline and negatively influence the performance of your model.\n",
        "\n",
        "There are many sources of possible errors:\n",
        "- Wrong language (do not overestimate multi-language claims)\n",
        "- Wrong character set (missing characters)\n",
        "- Wrong tokenization leading to unknown or corrupted tokens for which no embeddings exist or that cannot be dealt with sub-words\n",
        "- No relevant context for sub-word embeddings\n",
        "- Missing embeddings for chemical formulas, DNA/RNA, abbreviations, etc  \n",
        "- Missing embeddings in general (out of vocabulairy errors) and limited sub-words\n",
        "- Bias in embeddings\n",
        "- ....\n",
        "\n",
        "One can state  that there are many ways to do this wrong and a few ways to do it right! In this tutorial we will look at a few of these problems and how to address them.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yams1sh5wljY"
      },
      "source": [
        "## Word2Vec\n",
        "In this section, we will focus on word representations. We will train a Word2Vec model from scratch, by using the same dataset as before. In this way, we try to compare the two datasets and see if we can find differences between words in a negative setting vs words in a positive setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br2FrWAyzZtG"
      },
      "source": [
        "Word2Vec learns its word embeddings by looking inside the documents and checking the nearby words. The core idea behind this is that similar words are nearby in a sentence.\n",
        "The most common implementation for Word2Vec in Python is the one by [gensim](https://radimrehurek.com/gensim/models/word2vec.html). We can compute the embeddings by passing our documents as sentences to the model. Then to get an embedding, we just index the models word vectors with our needed embedding:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words, movie_reviews as mr\n",
        "nltk_words = set(words.words())\n",
        "import string\n",
        "from collections import Counter\n",
        "def remove_punct(word):\n",
        "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "    return word if word in nltk_words else ''\n",
        "all_words = Counter(filter(remove_punct, mr.words()))\n",
        "all_words.most_common(10)\n",
        "\n",
        "documents = [(list(filter(remove_punct, mr.words(f))), mr.categories(f)) for f in mr.fileids()]\n",
        "print(\"Total number of documents:\", len(documents))\n",
        "print(\"Total number of words in first document:\", len(documents[0][0]))\n"
      ],
      "metadata": {
        "id": "Po9IkEIR5Qeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6253a1c8"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWcCTa7Vx28V"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=[doc for doc, cat in documents])\n",
        "word_vectors = model.wv\n",
        "print(word_vectors['the'])\n",
        "print(\"dimensionality word vectors:\"+ str(word_vectors['the'].shape))\n",
        "print(\"Size of vocabulary: \" + str(len(word_vectors.key_to_index)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUWGDYUc0qFQ"
      },
      "source": [
        "We can find the most similar vector nearby a word using `most_similar`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbk1ZsaUzrGe"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar('king')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu_9uBVx3Mfu"
      },
      "source": [
        "And we can even do arithmetic with it. The most famous example of this is the `king + man - woman = queen` analogy. By adding the vector of king and man to each other, and subtracting the vector of woman, we should get the queen vector. Lets try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrD-Erlzzuvb"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=['king','woman'],negative=['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQD-O4P3199"
      },
      "source": [
        "We get queen as the second most similar vector. We only trained our word2vec model on our reviews dataset which is a small dataset for word2vec standards, so that makes sense.\n",
        "\n",
        "Lastly, lets plot the data. For this, we need to represent our vectors as a 2-d space. For this, we need a dimensionality reduction technique, such as PCA or t-SNE. We use t-SNE (invented by someone who did the same master as you are doing!). It might take a while to compute the vectors below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm5VC75O4l8U"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "vectors = tsne.fit_transform(np.asarray(model.wv.vectors))\n",
        "x, y = zip(*vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMg0YUpX8_0R"
      },
      "outputs": [],
      "source": [
        "len(x), len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K__i6RQ7Jzf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE (t-distributed Stochastic Neighbor Embedding) was introduced by Laurens van der Maaten and Geoffrey Hinton.\n",
        "\n",
        "It was published in the paper:\n",
        "\n",
        "Laurens van der Maaten and Geoffrey Hinton. â€œVisualizing Data using t-SNE.â€ Journal of Machine Learning Research (JMLR), 2008.\n",
        "\n",
        "Laurens van der Maaten is Dutch and did his academic training in the Netherlands:\n",
        "\n",
        "ðŸŽ“ M.Sc. in Artificial Intelligence â€“ Maastricht University\n",
        "\n",
        "ðŸŽ“ Ph.D. in Computer Science â€“ Tilburg University (Netherlands)\n",
        "\n",
        "After his PhD he worked as a postdoctoral researcher at Delft University of Technology, then moved to Microsoft Research and later to Facebook AI Research (FAIR), where he became well known in the machine learning and computer vision community."
      ],
      "metadata": {
        "id": "ebDifjMqHbtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Word2Vec\n",
        "Word2Vec actually works best when using a pretrained word vectors. This means that we would not put in data in the model to train a good representation, but we rely on external researchers that have already trained such a system on so much data the word vectors have a good representation already.\n",
        "\n",
        "We now will use glove vectors. We can import such model like so. It might take a while to download them."
      ],
      "metadata": {
        "id": "QdtYf5ehuhM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "glove = gensim.downloader.load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "cO1sUsQOu2x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove[\"king\"]"
      ],
      "metadata": {
        "id": "PTBHPYXQvmTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0DHPtgXpXf_"
      },
      "source": [
        "### Exercise 1: Limitations of basic Word Embeddings such as Word2Vec and Glove\n",
        "\n",
        "Using all our `documents`, get the `glove` pretrained word vector for every word, take the average over all word vectors for each document.\n",
        "\n",
        "Here is the code to do that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_vectors = []\n",
        "for doc, cat in documents:\n",
        "    word_vectors_for_doc = []\n",
        "    for word in doc:\n",
        "        try:\n",
        "            # Get the glove vector for the word\n",
        "            vector = glove[word]\n",
        "            word_vectors_for_doc.append(vector)\n",
        "        except KeyError:\n",
        "            # Handle words not in the glove vocabulary (skip or use a zero vector)\n",
        "            pass  # Skipping words not in vocabulary\n",
        "\n",
        "    if word_vectors_for_doc:\n",
        "        # Calculate the average vector for the document\n",
        "        avg_vector = np.mean(word_vectors_for_doc, axis=0)\n",
        "        document_vectors.append((avg_vector, cat))\n",
        "    else:\n",
        "        # Handle empty documents after removing words not in vocabulary\n",
        "        pass # Skipping empty documents\n",
        "\n",
        "print(\"Number of documents with average vectors:\", len(document_vectors))\n",
        "# You can now use document_vectors for further analysis"
      ],
      "metadata": {
        "id": "NH_M2BEdK9Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3087499a"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Take a few pairs of document vectors and calculate cosine similarity\n",
        "# For example, comparing the first two documents\n",
        "if len(document_vectors) >= 2:\n",
        "    vec1 = document_vectors[0][0]\n",
        "    vec2 = document_vectors[1][0]\n",
        "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "    print(f\"Cosine similarity between document 1 and document 2: {similarity}\")\n",
        "\n",
        "# You can compare other pairs as well\n",
        "if len(document_vectors) >= 10 and len(document_vectors) < 2000:\n",
        "    vec_pos = document_vectors[0][0] # Assuming the first document is from a positive review\n",
        "    vec_neg = document_vectors[1000][0] # Assuming the 1001th document is from a negative review\n",
        "    similarity_pos_neg = cosine_similarity([vec_pos], [vec_neg])[0][0]\n",
        "    print(f\"Cosine similarity between a positive and a negative review document: {similarity_pos_neg}\")\n",
        "elif len(document_vectors) >= 2000:\n",
        "    vec_pos = document_vectors[0][0] # Assuming the first document is from a positive review\n",
        "    vec_neg = document_vectors[1000][0] # Assuming the 1001th document is from a negative review\n",
        "    similarity_pos_neg = cosine_similarity([vec_pos], [vec_neg])[0][0]\n",
        "    print(f\"Cosine similarity between a positive and a negative review document: {similarity_pos_neg}\")\n",
        "else:\n",
        "    print(\"Not enough documents to compare positive and negative reviews.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see: The cosine similarity is close to 1 for all documents because we calculated the average of all word vectors within each document. When you average many vectors, especially in longer documents, the resulting average vector tends to point in a similar general direction in the vector space, regardless of the specific nuances of the document's content. This is because the most frequent words in the dataset have a large influence on the average, making the document vectors converge towards a similar central point in the embedding space. This highlights one of the limitations of simply averaging word embeddings to represent documents."
      ],
      "metadata": {
        "id": "3YAfTl6uIj5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Questions:\n",
        "\n",
        "1a. Do the vectors differ a lot?\n",
        "\n",
        "1b. When one has a number of very long documents, what will the vectors average to?\n",
        "\n",
        "1c. Do you think this is a usefull representation of a document when using a task such as text classification?\n",
        "\n",
        "1d. What kind of errors will you observe when a Word2Vec or Glove model for English is used on German?\n",
        "\n",
        "1e. What will happen when certain tokens do not have an embedding in Word2Vec or Glove?  "
      ],
      "metadata": {
        "id": "jLJiu-5bI2MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias in Word2Vec\n",
        "One of the problems with Word2Vec (and with machine learning in general) is that there is lots of biases assumed by the model. Examples of biases that can be harmful when using these algorithms include gender bias and ethnicity bias. Lets check for example what happens if we take the female equivalent of `doctor`:"
      ],
      "metadata": {
        "id": "7Rj4cz1zMCeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.most_similar(positive=['doctor','woman'],negative=['man'])"
      ],
      "metadata": {
        "id": "mAjcNaqcMF_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2: Bias in Word2Vec\n",
        "\n",
        "Here are other examples of bias in Glove and in Word2Vec."
      ],
      "metadata": {
        "id": "jDdSAOEuT0b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.most_similar(positive=['programmer','woman'],negative=['man'])"
      ],
      "metadata": {
        "id": "TIVT4MwNUQ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove.most_similar(positive=['soldier','woman'],negative=['man'])\n"
      ],
      "metadata": {
        "id": "egBx2f1LJX-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove.most_similar(positive=['professor','woman'],negative=['man'])"
      ],
      "metadata": {
        "id": "nTbtEXtRJd0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "qGeOKjbPmc46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arrive at the state-of-the-art, Transformers models!\n",
        "\n",
        "Several (not all) of the problems observed in Word2Vec and Glove are addressed with the word embeddings in the Transformer model and in BERT in particular. The most important one being that so-called homonyms will get different embeddings based ont he context of a word in a sentence.\n",
        "\n",
        "Although in another course we go deeper into Transformers itself, in this section we will go through representing our dataset as vectors. We do this again with the use of a pretrained model, for example BERT. We use the `transformers` library from HuggingFace to download the model, and use it on our data. Lets install the library first and import the model.\n",
        "\n",
        "In this section we will use Sentence Transformers library, which is a popular way of calculating embeddings for sentences using transformer models. See the documentation of the library [here](https://www.sbert.net/).\n",
        "In essence, this library basically also uses BERT-based models, but uses a mean pooling algorithm to average the embeddings out over its tokens. Its also more efficient, it would take some time using BERT to compute all the embeddings for every document in our reviews dataset, Sentence Transformers is optimized to do such task.\n",
        "\n",
        "We start with downloading the library using `pip` and importing a model."
      ],
      "metadata": {
        "id": "LmZj5iBvmhOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "XvSjvFzQm-g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can encode any sentence like this:"
      ],
      "metadata": {
        "id": "DRRDncajnI0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embedding = sentence_model.encode(\"the quick brown fox jumps over the lazy dog\")\n",
        "sentence_embedding.shape"
      ],
      "metadata": {
        "id": "PGubLNl4nBwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now get a vector of 384 instead of a matrix of 11 by 768. This makes it much easier to deal with."
      ],
      "metadata": {
        "id": "NE9xT8UJnR5s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQhSjlX8LrSF"
      },
      "source": [
        "## BERT Models\n",
        "In this section, we will learn how to extract embeddings from the pre-trained BERT. Consider the sentence 'I love Maastricht'. Let's see how to obtain the contextualized word embedding of all the words in the sentence using the pre-trained BERT model with Hugging Face's transformer library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JWnz4MpfEd5L"
      },
      "outputs": [],
      "source": [
        "!pip install -qq transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make sure we get the same results every time we run the same code in the notebook (reproducability). We can do this by setting the seeds of the packages we use. Like this:"
      ],
      "metadata": {
        "id": "WCv1w4ydEd5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "LaAOvLlGEd5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Tokenizers"
      ],
      "metadata": {
        "id": "pvnQiWqZEd5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can tokenize a piece of text by importing the tokenizer from the `transformers` library. Then to tokenize we use the `tokenize()` function. This function splits the texts based on the items in the vocabulary."
      ],
      "metadata": {
        "id": "FaGOnN4oEd5M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_k8AHt0LrSH"
      },
      "source": [
        "Lets consider the `HuggingFace` library. We can check all the available pre-trained models [here](https://huggingface.co/models). For BERT, these models we can filter down on the `bert` [tag](https://huggingface.co/models?other=bert). For now, we use the [bert-base-uncased](https://huggingface.co/bert-base-uncased) model. As the name suggests, it a BERT with 12 encoders and it is trained with uncased tokens. The representation size will be 768. The `uncased` means that we have only lowercase letters in our tokenizer.\n",
        "\n",
        "We can download and load the pretrained model like this. Lets look how the model is implemented. Can you notice the 12 layers/encoders of the model? Also notice the different type of inputs we have in the embedding layer. The word embedding, that converts the token ids we have (30522 of them) into 768. Same holds for the position embeddings and the token_type_embeddings we could put in."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "p_hMv2uZe35T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swqqjz08_FKj"
      },
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8El2dcG_FKs"
      },
      "source": [
        "## Preprocessing the input\n",
        "Define the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8Hkc5o3B_FKu"
      },
      "outputs": [],
      "source": [
        "sentence = 'I love AI'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVn3-2XM_FKv"
      },
      "source": [
        "Tokenize the sentence and obtain the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4Ui-3oWr_FKw"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSYRLcvV_FKy"
      },
      "source": [
        "Let's print the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGHtPj5H_FKz"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The [CLS] and [SEP] tokens:**\n",
        "\n",
        "[CLS] (classification token):\n",
        "A special token added at the beginning of every input sequence.\n",
        "\n",
        "During pre-training, BERT learns to use the hidden state at the [CLS] position to represent the entire sequence.\n",
        "\n",
        "For classification tasks (e.g., sentiment, NLI, topic detection), the final hidden state of [CLS] is passed to a classifier head.\n",
        "\n",
        "[SEP] (separator token):\n",
        "Marks the end of a single sequence or the boundary between two sequences.\n",
        "\n",
        "Used in tasks with sentence pairs (e.g., question + passage, entailment).\n",
        "\n",
        "Even for single sentences, BERT expects a [SEP] to indicate sequence end because it was trained that way."
      ],
      "metadata": {
        "id": "d75wli2AKywM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why itâ€™s needed**\n",
        "\n",
        "Matches pre-training format\n",
        "BERT was trained on inputs like:\n",
        "\n",
        "[CLS] sentence A tokens [SEP] sentence B tokens [SEP]\n",
        "\n",
        "\n",
        "If you donâ€™t insert these tokens, the input format wonâ€™t match what the model learned, and performance drops.\n",
        "\n",
        "Provides a fixed â€œsequence embeddingâ€\n",
        "The [CLS] tokenâ€™s final hidden vector\n",
        "â„Ž\n",
        "[CLS]\n",
        "h\n",
        "[CLS]\n",
        "\tâ€‹\n",
        "\n",
        " acts as a compact embedding for the entire sequence â€” the classifier head expects it.\n",
        "\n",
        "Helps with segment information\n",
        "The [SEP] token signals where one sequence ends and the next begins, enabling the model to learn relationships (e.g., in entailment tasks).\n",
        "\n",
        "Maintains compatibility with position and segment embeddings\n",
        "BERT adds token type embeddings (segment A vs B) and positional encodings; [CLS] and [SEP] help define these boundaries."
      ],
      "metadata": {
        "id": "WPES4ZrQK-BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must add [CLS] and [SEP] because BERTâ€™s encoder and downstream heads were trained to expect them.\n",
        "[CLS] provides a pooled representation for classification tasks, and [SEP] clearly signals sequence boundaries â€” critical for pair tasks and general consistency."
      ],
      "metadata": {
        "id": "C9iz0hi5LdjM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8VgpcM_FK0"
      },
      "source": [
        "Now, we will add the `[CLS]` token at the beginning and `[SEP]` token at the end of the tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3CpARvDc_FK0"
      },
      "outputs": [],
      "source": [
        "tokens = ['[CLS]'] + tokens + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbFEyrk7_FK2"
      },
      "source": [
        "Let's look at our updated tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaf_JScw_FK3"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVD29L7A_FK3"
      },
      "source": [
        "As we can observe, we have `[CLS]` token at the beginning and sep token at the end of our tokens list. We can also observe that length of our tokens is 5.\n",
        "\n",
        "Say, we need to keep the length of our tokens list to 7, then, in that case, we will add two `[PAD]` tokens at the end as shown in the following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqMI--rA_FK4"
      },
      "outputs": [],
      "source": [
        "tokens = tokens + ['[PAD]'] + ['[PAD]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEEHs7CV_FK6"
      },
      "source": [
        "Let's print our updated tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tKmh_hk_FK6"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Masking**\n",
        "\n",
        "Masking is a technique used in Transformer-based models (like BERT) where some input tokens are hidden (masked) during training so the model must predict them from context. Masking is a pre-training trick where some tokens are hidden and the model learns to recover them, forcing it to understand surrounding context and build strong word and sentence representations."
      ],
      "metadata": {
        "id": "6RCtFUSXLkJc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPd9TPWE_FK7"
      },
      "source": [
        "\n",
        "\n",
        "As we can observe, now we have the tokens list consists of `[PAD]` tokens and the length of our tokens list is 7.\n",
        "\n",
        "Next, we create the attention mask. The attention mask is there to let the model know which items should be taken into account when calculating the attentions. Since `[PAD]` tokens are just to pad the string and do not have any semantic meaning, we should let the model know not to take them into account.\n",
        "\n",
        "We set the attention mask value to 1 if the token is not a `[PAD]` token else we will set the attention mask to 0 as shown below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3SbiaBws_FK8"
      },
      "outputs": [],
      "source": [
        "attention_mask1 = [1 if i!= '[PAD]' else 0 for i in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SELKgxsG_FK9"
      },
      "source": [
        "Let's print the attention_mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmtZKFFq_FK-"
      },
      "outputs": [],
      "source": [
        "print(attention_mask1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvtvC8HW_FK-"
      },
      "source": [
        "\n",
        "As we can observe, we have attention mask values 0 at the position where have `[PAD]` token and 1 at other positions.\n",
        "\n",
        "Next, we convert all the tokens to their token_ids as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KGbsHfxL_FK_"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQWwz_y_FK_"
      },
      "source": [
        "\n",
        "Let's have a look at the token_ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iih_wWeT_FLA"
      },
      "outputs": [],
      "source": [
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9RL1cBs_FLA"
      },
      "source": [
        "\n",
        "From the above output, we can observe that each token is mapped to a unique token id.\n",
        "\n",
        "Now, we convert the token_ids and attention_mask to tensors as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KQzsDUez_FLB"
      },
      "outputs": [],
      "source": [
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attention_mask1 = torch.tensor(attention_mask1).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQn5de_a_FLC"
      },
      "source": [
        "\n",
        "That's it. Next, we feed the token_ids and attention_mask to the pre-trained BERT model and get the embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYIDWAO-_FLD"
      },
      "source": [
        "## Getting the embedding\n",
        "\n",
        "As shown in the following code, we feed the token_ids, and attention_mask to the model and get the output of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rh-Ohh71_FLE"
      },
      "outputs": [],
      "source": [
        "print(token_ids)\n",
        "print(attention_mask1)\n",
        "last_hidden_state, pooler_output = model(token_ids, attention_mask = attention_mask1).to_tuple()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Kfvd3F_FLF"
      },
      "source": [
        "This output contains two keys or more keys, depending on the task at hand. In the case of `bert-base-uncased`, we get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx1RfsaU_FLF"
      },
      "outputs": [],
      "source": [
        "last_hidden_state.shape, pooler_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `pooler_output` is basically the `last_hidden_state`'s `[CLS]` token through the pooling module (linear layer + activation) at the end:"
      ],
      "metadata": {
        "id": "9Dsj4xWesaSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(pooler_output, model.pooler(last_hidden_state))"
      ],
      "metadata": {
        "id": "C4IjdcYzs6HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxFowIS_FLG"
      },
      "source": [
        "The size `[1,7,768]` indicates the `[batch_size, sequence_length, hidden_size]`.\n",
        "\n",
        "Our batch size is 1, the sequence length is the token length, since we have 7 tokens, the sequence length is 7, and the hidden size is the representation (embedding) size and it is 768 for the BERT-base model.\n",
        "\n",
        "We can obtain the representation of each token as:\n",
        "\n",
        "- `output.last_hidden_state[0][0]` gives the representation of the first token which is `[CLS]`\n",
        "- `output.last_hidden_state[0][1]` gives the representation of the second token which is 'I'\n",
        "- `output.last_hidden_state[0][2]` gives the representation of the third token which is 'love'.\n",
        "\n",
        "We can also index it with `output.last_hidden_state[:, 0, :]`. In this way, we specify that we want to have the zeroth element on the second index, we get a matrix in return with both the batch size and the embedding size (since we put a colon there).\n",
        "\n",
        "In this way, we can obtain the contextual representation of all the tokens. This is basically the contextualized word embeddings of all the words in the given sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3: BERT Context Sensitive Embeddings\n",
        "\n",
        "Now we will write your own code. Put the following text (the one defined under here with three times `bank` in it) to the tokenizer, add `[CLS]` and `[SEP]` values to it and pass it through the model. Get the vector representations of each of the three tokens of `bank` in the sentence and compare them.\n",
        "\n",
        "\n",
        "You can see how this differs from putting the text through a GloVe/Word2Vec model as there is now way to distinguish between different meanings for a word such as 'bank'.\n",
        "\n",
        "\n",
        "Putting a string with only \"bank\" (add `[CLS]` and `[SEP]`) without any context through the tokenizer and model wills how you that this will result in the same vectors for 'bank'. This is in the last coding example."
      ],
      "metadata": {
        "id": "zzsYzhiRaW1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Add [CLS] and [SEP] tokens\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "\n",
        "# Convert tokens to token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Convert token IDs to a tensor and add a batch dimension\n",
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "\n",
        "# Get the model output\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    last_hidden_state, pooler_output = model(token_ids).to_tuple()\n",
        "\n",
        "# Find the indices of the \"bank\" tokens\n",
        "bank_indices = [i for i, token in enumerate(tokens) if token == 'bank']\n",
        "\n",
        "# Get the vector representations for each \"bank\" token\n",
        "bank_vectors = [last_hidden_state[0, i, :] for i in bank_indices]\n",
        "\n",
        "# Print the vectors (optional)\n",
        "# for i, vector in enumerate(bank_vectors):\n",
        "#     print(f\"Vector for 'bank' instance {i+1}: {vector}\")\n",
        "\n",
        "# You can now compare these vectors (e.g., using cosine similarity)\n",
        "\n",
        "print(text)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "if len(bank_vectors) >= 2:\n",
        "    similarity_1_2 = cosine_similarity(bank_vectors[0].unsqueeze(0), bank_vectors[1].unsqueeze(0))[0][0]\n",
        "    print(f\"Cosine similarity between the first and second 'bank': {similarity_1_2}\")\n",
        "\n",
        "if len(bank_vectors) >= 3:\n",
        "    similarity_1_3 = cosine_similarity(bank_vectors[0].unsqueeze(0), bank_vectors[2].unsqueeze(0))[0][0]\n",
        "    print(f\"Cosine similarity between the first and third 'bank': {similarity_1_3}\")\n",
        "    similarity_2_3 = cosine_similarity(bank_vectors[1].unsqueeze(0), bank_vectors[2].unsqueeze(0))[0][0]\n",
        "    print(f\"Cosine similarity between the second and third 'bank': {similarity_2_3}\")"
      ],
      "metadata": {
        "id": "VfS11L-tMIJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector for \"bank\" from the glove model\n",
        "bank_vector_1 = glove[\"bank\"]\n",
        "\n",
        "# Access the vector for \"bank\" again - it will be the same\n",
        "bank_vector_2 = glove[\"bank\"]\n",
        "\n",
        "# You can compare them to show they are identical\n",
        "print(f\"Are the two 'bank' vectors from Glove identical? {np.array_equal(bank_vector_1, bank_vector_2)}\")\n",
        "\n",
        "# To further illustrate, let's get vectors for \"bank\" in different sentences\n",
        "# Note: We can't pass sentences to glove directly, only individual words.\n",
        "# This limitation is what BERT overcomes. But we can show that calling glove[\"bank\"]\n",
        "# always returns the same vector.\n",
        "\n",
        "# Even if we imagine the context, the glove vector for \"bank\" remains the same.\n",
        "print(\"\\nGlove vector for 'bank' (financial institution context):\")\n",
        "print(glove[\"bank\"][:10]) # Print first 10 elements for brevity\n",
        "\n",
        "print(\"\\nGlove vector for 'bank' (river bank context):\")\n",
        "print(glove[\"bank\"][:10]) # Print first 10 elements for brevity\n",
        "\n",
        "# The vectors are the same because Glove does not consider sentence context."
      ],
      "metadata": {
        "id": "To1k4kD1Mofo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sentence with only \"bank\"\n",
        "text_isolated = \"bank\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens_isolated = tokenizer.tokenize(text_isolated)\n",
        "\n",
        "# Add [CLS] and [SEP] tokens\n",
        "tokens_isolated = ['[CLS]'] + tokens_isolated + ['[SEP]']\n",
        "\n",
        "# Convert tokens to token IDs\n",
        "token_ids_isolated = tokenizer.convert_tokens_to_ids(tokens_isolated)\n",
        "\n",
        "# Convert token IDs to a tensor and add a batch dimension\n",
        "token_ids_isolated = torch.tensor(token_ids_isolated).unsqueeze(0)\n",
        "\n",
        "# Get the model output\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    last_hidden_state_isolated, pooler_output_isolated = model(token_ids_isolated).to_tuple()\n",
        "\n",
        "# Get the vector representation for the \"bank\" token\n",
        "# The index for \"bank\" is 1 because of the [CLS] token at the beginning\n",
        "bank_vector_isolated = last_hidden_state_isolated[0, 1, :]\n",
        "\n",
        "print(\"Vector for 'bank' in isolation:\")\n",
        "# print(bank_vector_isolated) # Uncomment to print the full vector\n",
        "\n",
        "# Now compare this vector to the contextualized vectors from the previous example (Exercise 3a)\n",
        "# Assuming 'bank_vectors' from Exercise 3a is still available\n",
        "\n",
        "if 'bank_vectors' in locals() and len(bank_vectors) >= 3:\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    similarity_isolated_1 = cosine_similarity(bank_vector_isolated.unsqueeze(0), bank_vectors[0].unsqueeze(0))[0][0]\n",
        "    print(f\"Cosine similarity between 'bank' in isolation and the first 'bank' (vault): {similarity_isolated_1}\")\n",
        "\n",
        "    similarity_isolated_2 = cosine_similarity(bank_vector_isolated.unsqueeze(0), bank_vectors[1].unsqueeze(0))[0][0]\n",
        "    print(f\"Cosine similarity between 'bank' in isolation and the second 'bank' (robber): {similarity_isolated_2}\")\n",
        "\n",
        "    similarity_isolated_3 = cosine_similarity(bank_vector_isolated.unsqueeze(0), bank_vectors[2].unsqueeze(0))[0][0]\n",
        "    print(f\"Cosine similarity between 'bank' in isolation and the third 'bank' (river): {similarity_isolated_3}\")\n",
        "else:\n",
        "    print(\"Contextualized bank vectors from Exercise 3a not found. Please run Exercise 3a code first.\")"
      ],
      "metadata": {
        "id": "7wahQQlfNce5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with Tokenization Problems for Chemical Formulas or RNA/DNA\n",
        "\n",
        "In the beginning of this tutorial, we explained that there are many potenial sources of problems in Word2Vec and Glove:\n",
        "\n",
        "- Wrong language (do not overestimate multi-language claims)\n",
        "- Wrong character set (missing characters)\n",
        "- Wrong tokenization leading to unknown or corrupted tokens for which no embeddings exist or that cannot be dealt with sub-words\n",
        "- No relevant context for sub-word embeddings\n",
        "- Missing embeddings for chemical formulas, DNA/RNA, abbreviations, etc\n",
        "- Missing embeddings in general (out of vocabulairy errors) and limited sub-words\n",
        "- Bias in embeddings\n",
        "\n",
        "Language and character set detection in combination with special embeddings per language can solve the first two problems. Bias is caused by pre-training the models with text that contains various types of unwanted forms of bias.\n",
        "\n",
        "ChemBERTa is a specialized model dealing with chemical formulas, DNABERT is a model dealing with RNA/DNA sequences. Both have specialized tokenizers, embeddings and special context embeddings derived from pre-training the models with relevant text data. BIOBERT is a good example of a BERT model specialized for Bio Medical text.\n",
        "\n",
        "You can run the BioBERT Colab Tutorial for Medical Named Entity Recognition that can be found here: https://colab.research.google.com/github/eugenesiow/practical-ml/blob/master/notebooks/Named_Entity_Recognition_BC5CDR.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "54AuOXXTYy0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hereunder is an exmample of a tokenization going wrong."
      ],
      "metadata": {
        "id": "t6qjG5gHOh2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: A made-up word that might not be in the vocabulary\n",
        "text_madeup = \"This is a supercalifragilisticexpialidocious word.\"\n",
        "tokens_madeup = tokenizer.tokenize(text_madeup)\n",
        "print(f\"Original text: '{text_madeup}'\")\n",
        "print(f\"Tokens: {tokens_madeup}\")\n",
        "print(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokens_madeup)}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Example 2: A chemical formula (BERT tokenizer is not specialized for this)\n",
        "text_chemical = \"The formula for water is H2O.\"\n",
        "tokens_chemical = tokenizer.tokenize(text_chemical)\n",
        "print(f\"Original text: '{text_chemical}'\")\n",
        "print(f\"Tokens: {tokens_chemical}\")\n",
        "print(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokens_chemical)}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Example 3: A string with special characters or symbols not in the vocabulary\n",
        "text_symbols = \"Let's analyze the data: @#$!.\"\n",
        "tokens_symbols = tokenizer.tokenize(text_symbols)\n",
        "print(f\"Original text: '{text_symbols}'\")\n",
        "print(f\"Tokens: {tokens_symbols}\")\n",
        "print(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokens_symbols)}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Example 4: A word that might be split into unexpected sub-words\n",
        "text_subword = \"Analyzing the data is crucial.\"\n",
        "tokens_subword = tokenizer.tokenize(text_subword)\n",
        "print(f\"Original text: '{text_subword}'\")\n",
        "print(f\"Tokens: {tokens_subword}\")\n",
        "print(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokens_subword)}\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "P0EQgugsONUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}